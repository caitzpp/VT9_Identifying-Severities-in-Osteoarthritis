{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "from utils.data_exploration_utils import investigate_data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#import hdbscan\n",
    "from sklearn.cluster import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a8e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "\n",
    "base_dir = config.RAW_DATA_PATH\n",
    "proc_dir = config.PROC_DATA_PATH\n",
    "save_dir = os.path.join(proc_dir, f\"{today}_hdbscan\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "folder = \"2025-07-03_data_exploration\"\n",
    "unpivoted = True\n",
    "\n",
    "if unpivoted:\n",
    "    df = pd.read_csv(os.path.join(proc_dir, folder, \"inmodi_data_personalinformation_unpivoted.csv\"))\n",
    "else:\n",
    "    df = pd.read_csv(os.path.join(proc_dir, folder, \"inmodi_data_personalinformation.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013de89c",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162d2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters\n",
    "params = {\n",
    "    'min_cluster_size': 5,\n",
    "    'min_samples': None,\n",
    "    'cluster_selection_epsilon': 0.0,\n",
    "    'max_cluster_size': None,\n",
    "    'metric': 'euclidean',\n",
    "    'metric_params': None,\n",
    "    'alpha': 1.0,\n",
    "    'algorithm': 'auto',\n",
    "    'leaf_size': 40,\n",
    "    'cluster_selection_method': 'eom',\n",
    "    'store_centers': 'centroid' #not default, but want to keep this\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4bdc86",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_filepath(base_path):\n",
    "    \"\"\"If file exists, append _2, _3, etc. until unique.\"\"\"\n",
    "    if not os.path.exists(base_path):\n",
    "        return base_path\n",
    "    base, ext = os.path.splitext(base_path)\n",
    "    counter = 2\n",
    "    new_path = f\"{base}_{counter}{ext}\"\n",
    "    while os.path.exists(new_path):\n",
    "        counter += 1\n",
    "        new_path = f\"{base}_{counter}{ext}\"\n",
    "    return new_path\n",
    "\n",
    "def save_results(df, clusterer, params, scaler, save_dir, filename):\n",
    "    df_filename = f\"{filename}.csv\"\n",
    "    results_df = pd.DataFrame({\n",
    "                    'record_id': df['record_id'],\n",
    "                    'cluster_label': clusterer.labels_,\n",
    "                    'probability': clusterer.probabilities_,\n",
    "                })\n",
    "    df_savepath = get_unique_filepath(os.path.join(save_dir, df_filename))\n",
    "    results_df.to_csv(df_savepath, index=False)\n",
    "\n",
    "    model_info = {\n",
    "            'df_savepath': df_savepath,\n",
    "            'params': params,\n",
    "            'scaler': scaler.__class__.__name__,\n",
    "            'n_clusters': len(set(clusterer.labels_)) - (1 if -1 in clusterer.labels_ else 0),\n",
    "            'centroids': clusterer.centroids_.tolist(),\n",
    "        }\n",
    "    model_info_filename = f\"{filename}_model_info.json\"\n",
    "    model_info_savepath = get_unique_filepath(os.path.join(save_dir, model_info_filename))\n",
    "    with open(model_info_savepath, 'w') as f:\n",
    "        json.dump(model_info, f, indent=4)\n",
    "    return os.path.basename(df_savepath).split('.')[0]\n",
    "\n",
    "def plot_hdbscan(X, labels, probabilities=None, parameters=None, ground_truth=False, ax=None, save_path = None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(10, 4))\n",
    "    labels = labels if labels is not None else np.ones(X.shape[0])\n",
    "    probabilities = probabilities if probabilities is not None else np.ones(X.shape[0])\n",
    "    # Black removed and is used for noise instead.\n",
    "    unique_labels = set(labels)\n",
    "    colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "    # The probability of a point belonging to its labeled cluster determines\n",
    "    # the size of its marker\n",
    "    proba_map = {idx: probabilities[idx] for idx in range(len(labels))}\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = [0, 0, 0, 1]\n",
    "\n",
    "        class_index = (labels == k).nonzero()[0]\n",
    "        for ci in class_index:\n",
    "            ax.plot(\n",
    "                X[ci, 0],\n",
    "                X[ci, 1],\n",
    "                \"x\" if k == -1 else \"o\",\n",
    "                markerfacecolor=tuple(col),\n",
    "                markeredgecolor=\"k\",\n",
    "                markersize=4 if k == -1 else 1 + 5 * proba_map[ci],\n",
    "            )\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    preamble = \"True\" if ground_truth else \"Estimated\"\n",
    "    title = f\"{preamble} number of clusters: {n_clusters_}\"\n",
    "    if parameters is not None:\n",
    "        parameters_str = \", \".join(f\"{k}={v}\" for k, v in parameters.items())\n",
    "        title += f\" | {parameters_str}\"\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480dfb27",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "scaler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08c8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nan Values\n",
    "df_nanids = investigate_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d20b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df_nanids, these are the columns with NaN values\n",
    "df_nanids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8c2ba5",
   "metadata": {},
   "source": [
    "## Choose Relevant columns, Check remaining columns for NaN values & remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6757540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['record_id', # id column\n",
    "        #'visit', 'side', \n",
    "        'pain', \n",
    "        'age', \n",
    "        # 'ce_height', \n",
    "        # 'ce_weight',\n",
    "       'ce_bmi', \n",
    "       'ce_fm', \n",
    "       'gender', \n",
    "       'OKS_score', \n",
    "       'UCLA_score', \n",
    "       'FJS_score',\n",
    "       'KOOS_pain', \n",
    "       'KOOS_symptoms', \n",
    "       'KOOS_sport', \n",
    "       'KOOS_adl', \n",
    "       'KOOS_qol'\n",
    "]\n",
    "\n",
    "df2 = df[cols].copy()\n",
    "\n",
    "df2_missingna = investigate_data(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1839b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataframe before dropping NaN values: \", df2.shape)\n",
    "df2 = df2.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1674a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataframe after dropping NaN values: \", df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b353fe3",
   "metadata": {},
   "source": [
    "## Create dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d38eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'gender' convert to int\n",
    "df2['is_male'] = df['gender'].apply(lambda x: 1 if x=='male' else 0)\n",
    "df2 = df2.drop(columns= 'gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41730d6a",
   "metadata": {},
   "source": [
    "## Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913fdb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_scaled = df2.copy()\n",
    "scaler = StandardScaler()\n",
    "X = df2_scaled.drop(columns=['record_id'])\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d9412",
   "metadata": {},
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7359b025",
   "metadata": {},
   "source": [
    "## Scaled X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5963913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = HDBSCAN(**params)\n",
    "clusterer = clusterer.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = save_results(df2, clusterer, params, scaler, save_dir, 'default_hdbscan_scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bff962",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hdbscan(X_scaled, clusterer.labels_, \n",
    "             probabilities=clusterer.probabilities_, \n",
    "             parameters={'parameters': 'default'},\n",
    "             save_path = os.path.join(save_dir, f\"{base_name}_plot.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dd6c5e",
   "metadata": {},
   "source": [
    "x if it's noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e96b70",
   "metadata": {},
   "source": [
    "## Non-Scaled X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21490999",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = HDBSCAN(**params)\n",
    "clusterer = clusterer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eca0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = save_results(df2, clusterer, params, scaler, save_dir, 'default_hdbscan')\n",
    "plot_hdbscan(X_scaled, clusterer.labels_, \n",
    "             probabilities=clusterer.probabilities_, \n",
    "             parameters={'parameters': 'default'},\n",
    "             save_path = os.path.join(save_dir, f\"{base_name}_plot.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
