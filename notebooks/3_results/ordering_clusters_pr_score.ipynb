{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde4da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import pandas as pd\n",
    "import config\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97e2983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_orderings_all_splits(df, cluster_column='cluster_label', label_name='KL-Score'):\n",
    "    \"\"\"\n",
    "    For each KL-based binary split, return PR-AUC for ALL permutations\n",
    "    of cluster label orderings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unique cluster IDs\n",
    "    clusters = sorted(df[cluster_column].unique())\n",
    "    K = len(clusters)\n",
    "\n",
    "    # All permutations of cluster label orderings\n",
    "    permutations = list(itertools.permutations(range(K)))\n",
    "\n",
    "    # Define severity splits\n",
    "    thresholds = {\n",
    "        \"auc_pr\":  lambda x: x > 0,   # KL > 0\n",
    "        \"auc_mid\": lambda x: x > 1,   # KL > 1\n",
    "        \"auc_mid2\": lambda x: x > 2,  # KL > 2\n",
    "        \"auc_sev\": lambda x: x == 4   # KL == 4\n",
    "    }\n",
    "\n",
    "    # Final output\n",
    "    results = {}\n",
    "\n",
    "    for key, condition in thresholds.items():\n",
    "        y_true = (condition(df[label_name])).astype(int).values\n",
    "\n",
    "        # Ensure binary\n",
    "        if not set(np.unique(y_true)).issubset({0,1}):\n",
    "            raise ValueError(f\"y_true for {key} split is not binary.\")    \n",
    "\n",
    "        split_results = []\n",
    "\n",
    "        # Evaluate AUC for EVERY permutation\n",
    "        for perm in permutations:\n",
    "            # Build mapping: cluster_id → severity_score_from_perm\n",
    "            mapping = {cluster: score for cluster, score in zip(clusters, perm)}\n",
    "\n",
    "            # Convert cluster label to continuous score\n",
    "            y_score = df[cluster_column].map(mapping).values.astype(float)\n",
    "\n",
    "            # Compute PR-AUC\n",
    "            precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "            pr_auc = auc(recall, precision)\n",
    "\n",
    "            split_results.append({\n",
    "                \"mapping\": mapping,\n",
    "                \"auc\": pr_auc\n",
    "            })\n",
    "\n",
    "        results[key] = split_results\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc877708",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE = 'ss'\n",
    "MOD_PREFIX = \"mod_smallimg3\"\n",
    "NEPOCH = 'latest'\n",
    "\n",
    "\n",
    "DATAPATH = config.OUTPUT_PATH\n",
    "base_dir = config.RAW_DATA_PATH\n",
    "img_path = config.SCHULTHESS_DATAPATH\n",
    "proc_dir = config.PROC_DATA_PATH\n",
    "\n",
    "# #for rawq:\n",
    "# feature = 'rawq'\n",
    "# folder = \"2025-11-19_hdbscan\"\n",
    "# run = \"run10\"  \n",
    "\n",
    "# feature = 'img_features'\n",
    "# #for img features:\n",
    "# folder = \"2025-09-12_hdbscan\"\n",
    "# folder_date = folder.split('_')[0]\n",
    "# run = \"run92\"\n",
    "\n",
    "# feature = 'img_raw'\n",
    "# folder = \"2025-09-13_hdbscan_img\"\n",
    "# run = \"run32\"\n",
    "\n",
    "feature = 'agg'\n",
    "folder = \"2025-08-11_hdbscan\"\n",
    "run = 'run150'\n",
    "\n",
    "anomalyscore_metric = \"centre_mean\"\n",
    "cluster_col = \"cluster_label\"\n",
    "\n",
    "folder_date = folder.split('_')[0]\n",
    "\n",
    "\n",
    "\n",
    "if feature == 'rawq':\n",
    "    filepath = os.path.join(proc_dir, folder, \"pipeline\", run)\n",
    "    hdbscan_df = pd.read_csv(os.path.join(filepath, f'pipeline_{run}_umap_hdbscan_scaled.csv'))\n",
    "elif feature == 'img_features' or feature == 'img_raw':\n",
    "        filepath = os.path.join(proc_dir, \"radiographic_features\", folder, run)\n",
    "        hdbscan_df = pd.read_csv(os.path.join(filepath, f'{folder}_{run}_umap_hdbscan_scaled.csv'))\n",
    "elif feature == 'agg':\n",
    "      filepath = os.path.join(proc_dir, folder, run)\n",
    "      hdbscan_df = pd.read_csv(os.path.join(filepath, f'{folder}_{run}_umap_hdbscan_scaled.csv'))\n",
    "        \n",
    "kl = pd.read_csv(os.path.join(base_dir,  \"brul_knee_annotations.csv\"))\n",
    "kl2 = pd.read_csv(os.path.join(base_dir, \"rosand1_knee_annotations.csv\"))\n",
    "mri = pd.read_csv(os.path.join(base_dir, '2025-09-25_mrismall.csv'))\n",
    "\n",
    "# with open(os.path.join(filepath, f'pipeline_{run}_umap_hdbscan_scaled_model_info.json')) as f:\n",
    "#     model_info= json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31344590",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_df = hdbscan_df.merge(kl, left_on = 'id', right_on='name', how='left', validate='one_to_one')\n",
    "hdbscan_df = hdbscan_df.merge(kl2, left_on = 'id', right_on='name', how='left', validate='one_to_one', suffixes=('', '2'))\n",
    "\n",
    "try:\n",
    "    hdbscan_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df = pd.read_csv(os.path.join(DATAPATH, 'outputs', 'dfs', 'ss', 'mod_smallimg3_ss_aggregated_scores.csv'))\n",
    "\n",
    "df['id_temp'] = df['id'].apply(lambda x: x.split('/')[-1])\n",
    "df['id'] = df['id_temp'].apply(lambda x: x.split('.')[0])\n",
    "df.drop(columns=['id_temp'], inplace=True)\n",
    "hdbscan_df = hdbscan_df.merge(df, on='id', how='left', validate='one_to_one')\n",
    "\n",
    "# if 'KL-Score'  is na, fill with 'KL-Score2'\n",
    "hdbscan_df['KL-Score'] = hdbscan_df['KL-Score'].fillna(hdbscan_df['KL-Score2'])\n",
    "hdbscan_df['KL-Score2'] = hdbscan_df['KL-Score2'].fillna(hdbscan_df['KL-Score'])\n",
    "\n",
    "hdbscan_df['KL-Score'].fillna(-1, inplace=True)\n",
    "hdbscan_df['KL-Score2'].fillna(-1, inplace=True)\n",
    "\n",
    "hdbscan_df_dropna = hdbscan_df.copy()\n",
    "hdbscan_df_dropna = hdbscan_df_dropna[hdbscan_df_dropna['KL-Score'] != -1]\n",
    "hdbscan_df_dropna = hdbscan_df_dropna[hdbscan_df_dropna['KL-Score2'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8515d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_df_dropna['cluster_label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20987c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = all_orderings_all_splits(hdbscan_df_dropna, 'cluster_label', 'KL-Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048dea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= hdbscan_df_dropna.copy()\n",
    "\n",
    "label_name='KL-Score'\n",
    "cluster_column='cluster_label'\n",
    "\n",
    "\n",
    "clusters = sorted(df[cluster_column].unique())\n",
    "K = len(clusters)\n",
    "\n",
    "# All permutations of cluster label orderings\n",
    "# permutations = list(itertools.permutations(range(K)))\n",
    "permutations = []\n",
    "\n",
    "for _ in range(50):\n",
    "    permutations.append(random.sample(clusters, len(clusters)))\n",
    "\n",
    "# Define severity splits\n",
    "thresholds = {\n",
    "    \"auc_pr\":  lambda x: x > 0,   # KL > 0\n",
    "    \"auc_mid\": lambda x: x > 1,   # KL > 1\n",
    "    \"auc_mid2\": lambda x: x > 2,  # KL > 2\n",
    "    \"auc_sev\": lambda x: x == 4   # KL == 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bd769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "i = 0\n",
    "for perm in permutations:\n",
    "    i += 1\n",
    "    mapping = {cluster: score for cluster, score in zip(clusters, perm)} #cluster: klscore\n",
    "\n",
    "    y_score = df[cluster_column].map(mapping).values.astype(float)\n",
    "\n",
    "    results[i] = {'perm' :mapping, 'recall-precision': {}}\n",
    "\n",
    "    for key, condition in thresholds.items():\n",
    "        y_true = (condition(df[label_name])).astype(int).values\n",
    "\n",
    "        # Ensure binary\n",
    "        if not set(np.unique(y_true)).issubset({0,1}):\n",
    "            raise ValueError(f\"y_true for {key} split is not binary.\")\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        pr_auc = float('{:.1f}'.format(pr_auc*100))\n",
    "\n",
    "        #print(f'{key} PR-AUC: {(pr_auc*100):.1f}')\n",
    "        results[i]['recall-precision'][key] = pr_auc\n",
    "        # results[i][key] = pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcda605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k(results, k=5):\n",
    "    # extract metric names from the first entry\n",
    "    sample_key = next(iter(results))\n",
    "    metrics = results[sample_key][\"recall-precision\"].keys()\n",
    "\n",
    "    top_k_indices = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        # Build a list of (index, value)\n",
    "        values = [(idx, res[\"recall-precision\"][metric]) \n",
    "                  for idx, res in results.items()]\n",
    "\n",
    "        # Sort by the value (descending = best values first)\n",
    "        values_sorted = sorted(values, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Select top k indices\n",
    "        top_k_indices[metric] = values_sorted[:k]\n",
    "\n",
    "    return top_k_indices\n",
    "top_k_results = get_top_k(results, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_k(results, k=5):\n",
    "    # get metric names from the first entry\n",
    "    sample_key = next(iter(results))\n",
    "    metrics = results[sample_key][\"recall-precision\"].keys()\n",
    "\n",
    "    for metric in metrics:\n",
    "        print(f\"\\n=== Top {k} permutations for {metric} ===\")\n",
    "\n",
    "        # Build list of (index, value, perm)\n",
    "        values = []\n",
    "        for idx, res in results.items():\n",
    "            auc_value = res[\"recall-precision\"][metric]\n",
    "            perm = res[\"perm\"]\n",
    "            values.append((idx, auc_value, perm))\n",
    "\n",
    "        # Sort by AUC descending\n",
    "        values_sorted = sorted(values, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Print top k\n",
    "        for rank, (idx, auc_value, perm) in enumerate(values_sorted[:k], start=1):\n",
    "            print(f\"\\n#{rank}  (perm index: {idx})\")\n",
    "            print(f\"AUC: {auc_value}\")\n",
    "            print(f\"Permutation mapping: {perm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075bcdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4957443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_k(results, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4bec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_orderings_auc_pr_and_sev(df, permutations, cluster_column='cluster_label', label_name='KL-Score'):\n",
    "    \"\"\"\n",
    "    Computes PR-AUC across all permutations of cluster orderings.\n",
    "    Returns ONLY the best permutation for:\n",
    "        • auc_pr  (KL > 0)\n",
    "        • auc_sev (KL == 4)\n",
    "    Also returns all scores in case needed.\n",
    "    \"\"\"\n",
    "\n",
    "    import itertools\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "    # Unique cluster IDs\n",
    "    clusters = sorted(df[cluster_column].unique())\n",
    "    K = len(clusters)\n",
    "\n",
    "    # # All permutations\n",
    "    # permutations = list(itertools.permutations(range(K)))\n",
    "\n",
    "    # Define severity splits\n",
    "    thresholds = {\n",
    "        \"auc_pr\":  lambda x: x > 0,   # KL > 0\n",
    "        \"auc_mid\": lambda x: x > 1,   # KL > 1\n",
    "        \"auc_mid2\": lambda x: x > 2,  # KL > 2\n",
    "        \"auc_sev\": lambda x: x == 4   # KL == 4\n",
    "    }\n",
    "\n",
    "    # Final dictionary\n",
    "    all_results = {}\n",
    "    best_results = {\n",
    "        \"auc_pr\":  {\"best_auc\": -1, \"best_mapping\": None},\n",
    "        \"auc_sev\": {\"best_auc\": -1, \"best_mapping\": None}\n",
    "    }\n",
    "\n",
    "    for key, condition in thresholds.items():\n",
    "        y_true = (condition(df[label_name])).astype(int).values\n",
    "\n",
    "        if not set(np.unique(y_true)).issubset({0,1}):\n",
    "            raise ValueError(f\"y_true for {key} split is not binary.\")\n",
    "\n",
    "        split_scores = []\n",
    "        \n",
    "        for perm in permutations:\n",
    "            # cluster → score mapping\n",
    "            mapping = {cluster: score for cluster, score in zip(clusters, perm)}\n",
    "\n",
    "            # score for PR curve\n",
    "            y_score = df[cluster_column].map(mapping).astype(float).values\n",
    "\n",
    "            precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "            pr_auc = auc(recall, precision)\n",
    "\n",
    "            split_scores.append({\n",
    "                \"mapping\": mapping,\n",
    "                \"auc\": pr_auc\n",
    "            })\n",
    "\n",
    "            # Track best only for auc_pr and auc_sev\n",
    "            if key in best_results:\n",
    "                if pr_auc > best_results[key][\"best_auc\"]:\n",
    "                    best_results[key][\"best_auc\"] = pr_auc\n",
    "                    best_results[key][\"best_mapping\"] = mapping\n",
    "\n",
    "        all_results[key] = split_scores\n",
    "\n",
    "    return {\n",
    "        \"all_results\": all_results,\n",
    "        \"best_auc_pr\": best_results[\"auc_pr\"],\n",
    "        \"best_auc_sev\": best_results[\"auc_sev\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcaf000",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = best_orderings_auc_pr_and_sev(df,permutations,  'cluster_label', 'KL-Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['best_auc_pr'])\n",
    "print(results['best_auc_sev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086410d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_orderings_aucroc_pr_and_sev(df, permutations, cluster_column='cluster_label', label_name='KL-Score'):\n",
    "    \"\"\"\n",
    "    Computes ROC-AUC across all permutations of cluster orderings.\n",
    "    Returns ONLY the best permutation for:\n",
    "        • auc_roc (KL > 0)\n",
    "        • auc_sev (KL == 4)\n",
    "    Also returns full results.\n",
    "    \"\"\"\n",
    "\n",
    "    import itertools\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    # Unique cluster IDs\n",
    "    clusters = sorted(df[cluster_column].unique())\n",
    "    K = len(clusters)\n",
    "\n",
    "    # # All permutations of orderings\n",
    "    # permutations = list(itertools.permutations(range(K)))\n",
    "\n",
    "    # Severity threshold definitions (same as before)\n",
    "    thresholds = {\n",
    "        \"auc_roc\": lambda x: x > 0,   # KL > 0\n",
    "        \"auc_mid\": lambda x: x > 1,   # KL > 1\n",
    "        \"auc_mid2\": lambda x: x > 2,  # KL > 2\n",
    "        \"auc_sev\": lambda x: x == 4   # KL == 4\n",
    "    }\n",
    "\n",
    "    # Storage\n",
    "    all_results = {}\n",
    "    best_results = {\n",
    "        \"auc_roc\": {\"best_auc\": -1, \"best_mapping\": None},\n",
    "        \"auc_sev\": {\"best_auc\": -1, \"best_mapping\": None}\n",
    "    }\n",
    "\n",
    "    for key, condition in thresholds.items():\n",
    "        y_true = (condition(df[label_name])).astype(int).values\n",
    "\n",
    "        # check binary validity\n",
    "        if not set(np.unique(y_true)).issubset({0,1}):\n",
    "            raise ValueError(f\"y_true for {key} split is not binary.\")\n",
    "\n",
    "        split_scores = []\n",
    "\n",
    "        # evaluate every cluster ordering\n",
    "        for perm in permutations:\n",
    "\n",
    "            # build mapping: cluster -> score (continuous)\n",
    "            mapping = {cluster: score for cluster, score in zip(clusters, perm)}\n",
    "            y_score = df[cluster_column].map(mapping).astype(float).values\n",
    "\n",
    "            # ROC curve\n",
    "            fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "\n",
    "            split_scores.append({\n",
    "                \"mapping\": mapping,\n",
    "                \"auc\": roc_auc\n",
    "            })\n",
    "\n",
    "            # Update ONLY for the two metrics we care about\n",
    "            if key in best_results:\n",
    "                if roc_auc > best_results[key][\"best_auc\"]:\n",
    "                    best_results[key][\"best_auc\"] = roc_auc\n",
    "                    best_results[key][\"best_mapping\"] = mapping\n",
    "\n",
    "        all_results[key] = split_scores\n",
    "\n",
    "    return {\n",
    "        \"all_results\": all_results,\n",
    "        \"best_auc_roc\": best_results[\"auc_roc\"],\n",
    "        \"best_auc_sev\": best_results[\"auc_sev\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef2773",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = best_orderings_aucroc_pr_and_sev(df, permutations, 'cluster_label', 'KL-Score')\n",
    "print(results['best_auc_roc'])\n",
    "print(results['best_auc_sev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_mapping(mapping):\n",
    "    return dict(sorted(mapping.items(), key=lambda x: x[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_auc_roc_mapping = sort_mapping(results[\"best_auc_roc\"][\"best_mapping\"])\n",
    "sorted_auc_sev_mapping = sort_mapping(results[\"best_auc_sev\"][\"best_mapping\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0981ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_in_custom_order(result_dict, key, cluster_order):\n",
    "    \"\"\"\n",
    "    Extract mapping from result_dict['best_auc_roc'] or ['best_auc_sev']\n",
    "    and return the mapping values in the order given by cluster_order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    result_dict : output of best_orderings_aucroc_pr_and_sev\n",
    "    key : 'best_auc_roc' or 'best_auc_sev'\n",
    "    cluster_order : list of cluster labels in the order you want\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of scores in the given cluster order.\n",
    "    \"\"\"\n",
    "\n",
    "    mapping = result_dict[key][\"best_mapping\"]\n",
    "\n",
    "    # invert: score -> cluster\n",
    "    inverse = {score: cluster for cluster, score in mapping.items()}\n",
    "\n",
    "    # return labels sorted by score\n",
    "    return [inverse[i] for i in sorted(inverse.keys())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319dd024",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_auc_sev_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf41c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_order = sorted(df['cluster_label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc871d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db86cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_in_custom_order(results, \"best_auc_sev\", cluster_order)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
