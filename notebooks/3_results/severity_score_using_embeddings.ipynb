{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a4e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import pandas as pd\n",
    "import config\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE = 'ss'\n",
    "MOD_PREFIX = \"mod_smallimg3\"\n",
    "NEPOCH = 'latest'\n",
    "\n",
    "\n",
    "DATAPATH = config.OUTPUT_PATH\n",
    "base_dir = config.RAW_DATA_PATH\n",
    "img_path = config.SCHULTHESS_DATAPATH\n",
    "proc_dir = config.PROC_DATA_PATH\n",
    "\n",
    "# #for rawq:\n",
    "feature = 'rawq'\n",
    "methods = 'comb_modalities'\n",
    "folder = \"2026-01-17_hdbscan\"\n",
    "run = \"run28\"  \n",
    "\n",
    "# folder = \"2025-11-19_hdbscan\"\n",
    "# run = \"run10\"  \n",
    "# mapping_list = [0, 4, 2, 3, -1, 1]\n",
    "\n",
    "# feature = 'img_features'\n",
    "# #for img features:\n",
    "# folder = \"2025-09-12_hdbscan\"\n",
    "# folder_date = folder.split('_')[0]\n",
    "# run = \"run92\"\n",
    "\n",
    "# feature = 'img_raw'\n",
    "# folder = \"2025-09-13_hdbscan_img\"\n",
    "# run = \"run32\"\n",
    "\n",
    "# feature = 'agg'\n",
    "# folder = \"2025-08-11_hdbscan\"\n",
    "# run = 'run150'\n",
    "\n",
    "anomalyscore_metric = \"centre_mean\"\n",
    "cluster_col = \"cluster_label\"\n",
    "\n",
    "folder_date = folder.split('_')[0]\n",
    "\n",
    "\n",
    "\n",
    "if feature == 'rawq':\n",
    "    filepath = os.path.join(proc_dir, folder, methods, run)\n",
    "    hdbscan_df = pd.read_csv(os.path.join(filepath, f'{methods}_{run}_umap_hdbscan_scaled.csv'))\n",
    "    if methods == 'comb_modalities':\n",
    "        hdbscan_df_test = pd.read_csv(os.path.join(filepath, f'{methods}_{run}_umap_hdbscan_scaled_test_results.csv'))\n",
    "elif feature == 'img_features' or feature == 'img_raw':\n",
    "        filepath = os.path.join(proc_dir, \"radiographic_features\", folder, run)\n",
    "        hdbscan_df = pd.read_csv(os.path.join(filepath, f'{folder}_{run}_umap_hdbscan_scaled.csv'))\n",
    "elif feature == 'agg':\n",
    "      filepath = os.path.join(proc_dir, folder, run)\n",
    "      hdbscan_df = pd.read_csv(os.path.join(filepath, f'{folder}_{run}_umap_hdbscan_scaled.csv'))\n",
    "        \n",
    "kl = pd.read_csv(os.path.join(base_dir,  \"brul_knee_annotations.csv\"))\n",
    "kl2 = pd.read_csv(os.path.join(base_dir, \"rosand1_knee_annotations.csv\"))\n",
    "mri = pd.read_csv(os.path.join(base_dir, '2025-09-25_mrismall.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f90c0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hdbscan_df.shape)\n",
    "print(hdbscan_df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fd4398",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_df = \"2025-09-11_data_exploration\"\n",
    "df_filename = \"inmodi_data_questionnaire_kl_woSC.csv\"\n",
    "df = pd.read_csv(os.path.join(proc_dir, folder_df, df_filename))\n",
    "df2 = hdbscan_df[hdbscan_df['id'].isin(df['name'])]\n",
    "print(len(df2))\n",
    "try:\n",
    "    df2_test = hdbscan_df_test[hdbscan_df_test['id'].isin(df['name'])]\n",
    "    print(len(df2_test))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e99187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_df = df2\n",
    "hdbscan_df_test = df2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f20e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = ['IM1512_2_left',\n",
    "#  'IM1512_2_right',\n",
    "#  'IM1567_2_left',\n",
    "#  'IM1567_2_right',\n",
    "#  'IM1578_2_left',\n",
    "#  'IM1578_2_right',\n",
    "#  'IM2511_1_left',\n",
    "#  'IM2511_1_right',\n",
    "#  'IM2569_1_left',\n",
    "#  'IM2569_1_right',\n",
    "#  'IM3013_1_right',\n",
    "#  'IM3019_1_right',\n",
    "#  'IM3020_1_left',\n",
    "#  'IM3020_1_right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9792b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = pd.read_csv(os.path.join(base_dir, 'questionnaires_raw.csv')) #?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e656fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in l:\n",
    "#     hdbscan_df[hdbscan_df['id'] == i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_df = hdbscan_df.merge(kl, left_on = 'id', right_on='name', how='left', validate='one_to_one')\n",
    "\n",
    "try:\n",
    "    hdbscan_df_test = hdbscan_df_test.merge(kl, left_on = 'id', right_on='name', how='left', validate='one_to_one')\n",
    "except:\n",
    "    pass\n",
    "# hdbscan_df = hdbscan_df.merge(kl2, left_on = 'id', right_on='name', how='left', validate='one_to_one', suffixes=('', '2'))\n",
    "\n",
    "try:\n",
    "    hdbscan_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    hdbscan_df_test.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df = pd.read_csv(os.path.join(DATAPATH, 'outputs', 'dfs', 'ss', 'mod_smallimg3_ss_aggregated_scores.csv'))\n",
    "\n",
    "df['id_temp'] = df['id'].apply(lambda x: x.split('/')[-1])\n",
    "df['id'] = df['id_temp'].apply(lambda x: x.split('.')[0])\n",
    "df.drop(columns=['id_temp'], inplace=True)\n",
    "hdbscan_df = hdbscan_df.merge(df, on='id', how='left', validate='one_to_one')\n",
    "\n",
    "try:\n",
    "    hdbscan_df_test = hdbscan_df_test.merge(df, on='id', how='left', validate='one_to_one')\n",
    "    hdbscan_df_test['KL-Score'].fillna(-1, inplace=True)\n",
    "    hdbscan_df_test_dropna = hdbscan_df_test.copy()\n",
    "    ids_test = hdbscan_df_test['id'].tolist()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# if 'KL-Score'  is na, fill with 'KL-Score2'\n",
    "# hdbscan_df['KL-Score'] = hdbscan_df['KL-Score'].fillna(hdbscan_df['KL-Score2'])\n",
    "# hdbscan_df['KL-Score2'] = hdbscan_df['KL-Score2'].fillna(hdbscan_df['KL-Score'])\n",
    "\n",
    "hdbscan_df['KL-Score'].fillna(-1, inplace=True)\n",
    "# hdbscan_df['KL-Score2'].fillna(-1, inplace=True)\n",
    "ids = hdbscan_df['id'].tolist()\n",
    "\n",
    "\n",
    "hdbscan_df_dropna = hdbscan_df.copy()\n",
    "# hdbscan_df_dropna = hdbscan_df_dropna[hdbscan_df_dropna['KL-Score'] != -1]\n",
    "# hdbscan_df_dropna = hdbscan_df_dropna[hdbscan_df_dropna['KL-Score2'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc5ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(filepath, f'{methods}_{run}_umap_hdbscan_scaled_model_info.json')) as f:\n",
    "    model_info= json.load(f)\n",
    "if methods == 'comb_modalities':\n",
    "    umap_info = model_info['params']['umap']\n",
    "    nneigh = umap_info['n_neighbors']\n",
    "    min_dist = umap_info['min_dist']\n",
    "    metric = umap_info['metric']\n",
    "\n",
    "    umap_folder = f'nneigh{nneigh}_mindist{min_dist}_metric{metric}'\n",
    "    umap_path = os.path.join(proc_dir, '2026-01-16_umap_scaler_values', 'comb_modalities', umap_folder)\n",
    "\n",
    "    embeddings_path = os.path.join(umap_path, 'X_umap_embeddings.npy')\n",
    "    embeddings = np.load(embeddings_path)\n",
    "\n",
    "    embeddings_path_test = os.path.join(filepath, 'X_test_umap_embeddings.npy')\n",
    "    embeddings_test = np.load(embeddings_path_test)\n",
    "else:\n",
    "    embeddings_path = os.path.join(filepath, \"X_umap_embeddings.npy\")\n",
    "    embeddings = np.load(embeddings_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf315df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = hdbscan_df['cluster_label'].unique().tolist()\n",
    "clusters.sort()\n",
    "\n",
    "healthy_cluster = 0\n",
    "healthy_centroid = embeddings[hdbscan_df['cluster_label'] == healthy_cluster].mean(axis=0)\n",
    "healthy_spread = np.mean(np.linalg.norm(embeddings[hdbscan_df['cluster_label'] == healthy_cluster]-healthy_centroid, axis=1))\n",
    "\n",
    "cluster_centroids = {}\n",
    "\n",
    "for cluster in clusters:\n",
    "    cluster_embeddings = embeddings[hdbscan_df['cluster_label'] == cluster]\n",
    "    cluster_centroid = cluster_embeddings.mean(axis=0)\n",
    "    spread = np.mean(np.linalg.norm(cluster_embeddings-cluster_centroid, axis=1))\n",
    "    cluster_centroids[cluster] = {\n",
    "        'centroid': cluster_centroid,\n",
    "        'distance': np.linalg.norm(cluster_centroid-healthy_centroid),\n",
    "        'spread': spread,\n",
    "        'norm_distance': np.linalg.norm(cluster_centroid - healthy_centroid)/(healthy_spread+spread)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a18fbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# healthy_mask = hdbscan_df['cluster_label'] == healthy_cluster\n",
    "# healthy_embeddings = embeddings[healthy_mask]\n",
    "# healthy_centroid = healthy_embeddings.mean(axis=0)\n",
    "\n",
    "# healthy_spread = np.mean(\n",
    "#     np.linalg.norm(\n",
    "#         healthy_embeddings - healthy_centroid,\n",
    "#         axis=1\n",
    "#     )\n",
    "# )\n",
    "# dist_to_healthy = cdist(embeddings, healthy_embeddings)\n",
    "\n",
    "# point_severity = dist_to_healthy.min(axis=1)\n",
    "\n",
    "# cluster_centroids = {}\n",
    "\n",
    "# for cluster in clusters:\n",
    "#     cluster_mask = hdbscan_df['cluster_label'] == cluster\n",
    "#     cluster_embeddings = embeddings[cluster_mask]\n",
    "\n",
    "#     # centroid (for reference / plotting)\n",
    "#     cluster_centroid = cluster_embeddings.mean(axis=0)\n",
    "\n",
    "#     # spread = mean distance to centroid\n",
    "#     spread = np.mean(\n",
    "#         np.linalg.norm(cluster_embeddings - cluster_centroid, axis=1)\n",
    "#     )\n",
    "\n",
    "#     # NEW: cluster distance = mean nearest-healthy distance\n",
    "#     cluster_distance = point_severity[cluster_mask].mean()\n",
    "#     print(f\"Cluster {cluster}: distance to healthy = {cluster_distance}, spread = {spread}\")\n",
    "\n",
    "#     # normalized distance\n",
    "#     denom = healthy_spread + spread\n",
    "#     norm_distance = cluster_distance / denom if denom > 0 else np.nan\n",
    "\n",
    "#     cluster_centroids[cluster] = {\n",
    "#         'centroid': cluster_centroid,\n",
    "#         'distance': cluster_distance,\n",
    "#         'spread': spread,\n",
    "#         'norm_distance': norm_distance\n",
    "#     }\n",
    "\n",
    "\n",
    "# # hdbscan_df['severity_score'] = point_severity\n",
    "\n",
    "# # hdbscan_df['severity_score_norm'] = np.where(\n",
    "# #     healthy_spread\n",
    "# #     + hdbscan_df['cluster_label'].map(\n",
    "# #         lambda c: cluster_centroids.get(c, {}).get('spread', np.nan)\n",
    "# #     ).values\n",
    "# #     > 0,\n",
    "# #     point_severity\n",
    "# #     / (\n",
    "# #         healthy_spread\n",
    "# #         + hdbscan_df['cluster_label'].map(\n",
    "# #             lambda c: cluster_centroids.get(c, {}).get('spread', np.nan)\n",
    "# #         ).values\n",
    "# #     ),\n",
    "# #     np.nan\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ef39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_distance(row, embeddings):\n",
    "    c = row['cluster_label']\n",
    "    point = embeddings[row.name]\n",
    "\n",
    "    raw_distance = np.linalg.norm(point - healthy_centroid)\n",
    "    norm = cluster_centroids[c]['spread'] + cluster_centroids[healthy_cluster]['spread']\n",
    "\n",
    "    return raw_distance / norm if norm >0 else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7036c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_df['severity_score'] = np.linalg.norm(embeddings - healthy_centroid, axis=1)\n",
    "hdbscan_df['severity_score_norm'] = hdbscan_df.apply(normalized_distance, axis = 1, embeddings= embeddings)\n",
    "hdbscan_df['severity_score_norm_cluster'] = hdbscan_df['cluster_label'].apply(lambda x: cluster_centroids[x]['norm_distance'])\n",
    "hdbscan_df['severity_score_manhatten'] = np.abs(np.linalg.norm(embeddings - healthy_centroid, ord=1, axis=1))\n",
    "hdbscan_df['severity_score_chebyshev'] = np.abs(np.linalg.norm(embeddings - healthy_centroid, ord=np.inf, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_df_test['severity_score'] = np.linalg.norm(embeddings_test - healthy_centroid, axis=1)\n",
    "hdbscan_df_test['severity_score_norm'] = hdbscan_df_test.apply(normalized_distance, axis = 1, embeddings= embeddings_test)\n",
    "hdbscan_df_test['severity_score_norm_cluster'] = hdbscan_df_test['cluster_label'].apply(lambda x: cluster_centroids[x]['norm_distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e208ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_df.groupby(\"cluster_label\")[\"severity_score_norm_cluster\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_df.groupby(\"cluster_label\")[\"severity_score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5e5f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_df.groupby(\"cluster_label\")[\"severity_score_norm\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a35b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_df.groupby(\"cluster_label\")[\"severity_score_manhatten\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ae4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_df.groupby(\"cluster_label\")[\"severity_score_chebyshev\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7182d9e7",
   "metadata": {},
   "source": [
    "Calculate for each cluster a middle, then calculate the distance from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd868566",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderd = sorted(cluster_centroids.items(), key=lambda x: x[1]['distance'])\n",
    "order_wonoised = [item for item in orderd if item[0] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce7b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = [0, 2, -1, 4, 3, 1]\n",
    "order_wonoise =  [0, 2, 4, 3, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8ee1a7",
   "metadata": {},
   "source": [
    "# Severity Score Behaviour for each cluster label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a4c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order = [0, 4, 2, 1]\n",
    "# order_wnoise = [0,4,2,-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85158bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_mapping = {old: new for new, old in enumerate(order_wonoise)}\n",
    "print(cluster_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b18122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_df['old_cluster'] = hdbscan_df['cluster_label']\n",
    "hdbscan_df['cluster_label'] = hdbscan_df['cluster_label'].map(cluster_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba6095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace cluster_label Nan with -1\n",
    "hdbscan_df['cluster_label'].fillna(-1, inplace=True)\n",
    "\n",
    "try:\n",
    "    hdbscan_df_test['old_cluster'] = hdbscan_df_test['cluster_label']\n",
    "    hdbscan_df_test['cluster_label'] = hdbscan_df_test['cluster_label'].map(cluster_mapping)\n",
    "    #replace cluster_label Nan with -1\n",
    "    hdbscan_df_test['cluster_label'].fillna(-1, inplace=True)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=hdbscan_df, x=\"cluster_label\", y=\"severity_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b7274",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=hdbscan_df_test, x=\"cluster_label\", y=\"severity_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e24c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_df['train_test'] = 'train'\n",
    "hdbscan_df_test['train_test'] = 'test'\n",
    "\n",
    "df = pd.concat([hdbscan_df, hdbscan_df_test], ignore_index=True)\n",
    "\n",
    "df.to_csv(os.path.join(filepath, f'{methods}_{run}_umap_hdbscan_severity_scores.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8cc683",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291e3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(filepath, f'{methods}_{run}_umap_hdbscan_scaled_model_info.json')) as f:\n",
    "    model_info= json.load(f)\n",
    "if methods == 'comb_modalities':\n",
    "    umap_info = model_info['params']['umap']\n",
    "    nneigh = umap_info['n_neighbors']\n",
    "    min_dist = umap_info['min_dist']\n",
    "    metric = umap_info['metric']\n",
    "\n",
    "    umap_folder = f'nneigh{nneigh}_mindist{min_dist}_metric{metric}'\n",
    "    umap_path = os.path.join(proc_dir, '2026-01-16_umap_scaler_values', 'comb_modalities', umap_folder)\n",
    "\n",
    "    embeddings_path = os.path.join(umap_path, 'X_umap_embeddings.npy')\n",
    "    embeddings = np.load(embeddings_path)\n",
    "\n",
    "    embeddings_path_test = os.path.join(filepath, 'X_test_umap_embeddings.npy')\n",
    "    embeddings_test = np.load(embeddings_path_test)\n",
    "else:\n",
    "    embeddings_path = os.path.join(filepath, \"X_umap_embeddings.npy\")\n",
    "    embeddings = np.load(embeddings_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd339285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_path = os.path.join(umap_path, 'SMOTE_generated_samples.csv')\n",
    "smote_df = pd.read_csv(smote_path)\n",
    "smote_embeddings = np.load(os.path.join(umap_path, 'X_umap_samp_embeddings.npy'))\n",
    "#load clusterer\n",
    "clusterer = joblib.load(os.path.join(filepath, f'{methods}_{run}_umap_hdbscan_scaled_clusterer.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d16006",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_df['cluster_label']= list(clusterer.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c9be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_df['old_label'] = smote_df['cluster_label']\n",
    "smote_df['cluster_label'] = smote_df['cluster_label'].map(cluster_mapping)\n",
    "smote_df['severity_score'] = np.linalg.norm(smote_embeddings - healthy_centroid, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f106a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_df.to_csv(os.path.join(filepath, f'{methods}_{run}_umap_hdbscan_smote_clusters.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f735981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import hdbscan\n",
    "# from umap import UMAP\n",
    "# import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_path = os.path.join(proc_dir, folder, 'pipeline', run, 'scaler.pkl')\n",
    "# umapmodel_path = os.path.join(proc_dir, folder, 'pipeline', run, 'umap_model.pkl')\n",
    "# hdbscan_path = os.path.join(proc_dir, folder, 'pipeline', run, 'pipeline_run10_umap_hdbscan_scaled_clusterer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60327425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = joblib.load(scaler_path)\n",
    "# umap_model = joblib.load(umapmodel_path)\n",
    "# clusterer = joblib.load(hdbscan_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ea5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_scaled = scaler.transform(smote.drop(columns=[ 'KL-Score']))\n",
    "# X_umap = umap_model.transform(X_scaled)\n",
    "\n",
    "# cluster_labels, strengths = hdbscan.approximate_predict(clusterer, X_umap)\n",
    "# smote['cluster_label'] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594dc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote['old_cluster'] = smote['cluster_label']\n",
    "# smote['cluster_label_name'] = smote['cluster_label'].astype(str).map(rating)\n",
    "# smote['cluster_label']=smote['cluster_label_name'].map(rating_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9177b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix = X_umap\n",
    "\n",
    "# healthy_centroid = embedding_matrix[smote['cluster_label'] == 0].mean(axis=0)\n",
    "# smote['severity_score'] = np.linalg.norm(embedding_matrix - healthy_centroid, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bacab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary = smote.groupby('cluster_label')['severity_score'].describe().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb708537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary.sort_values('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a04ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe4e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(\n",
    "    data=smote_df,\n",
    "    x=\"cluster_label\",\n",
    "    y=\"severity_score\",\n",
    "    palette=\"Set3\",\n",
    "    width=0.6,\n",
    "    showfliers=False,     # hide outliers\n",
    "    linewidth=1.5         # thicker box borders\n",
    ")\n",
    "\n",
    "# # add jittered points for visibility\n",
    "# sns.stripplot(\n",
    "#     data=smote,\n",
    "#     x=\"cluster_label\",\n",
    "#     y=\"severity_score\",\n",
    "#     color=\"black\",\n",
    "#     alpha=0.4,\n",
    "#     size=3,\n",
    "#     jitter=0.25\n",
    "# )\n",
    "\n",
    "plt.title(\"Severity Score Distribution per Cluster\", fontsize=16, pad=15)\n",
    "plt.xlabel(\"Cluster Label\", fontsize=13)\n",
    "plt.ylabel(\"Severity Score\", fontsize=13)\n",
    "plt.grid(axis=\"y\", alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fcf127",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(\n",
    "    data=smote_df,\n",
    "    x=\"old_label\",\n",
    "    y=\"severity_score\",\n",
    "    palette=\"Set3\",\n",
    "    width=0.6,\n",
    "    showfliers=False,     # hide outliers\n",
    "    linewidth=1.5         # thicker box borders\n",
    ")\n",
    "\n",
    "plt.title(\"Severity Score Distribution per Cluster\", fontsize=16, pad=15)\n",
    "plt.xlabel(\"Cluster Label\", fontsize=13)\n",
    "plt.ylabel(\"Severity Score\", fontsize=13)\n",
    "plt.grid(axis=\"y\", alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b57ffb",
   "metadata": {},
   "source": [
    "# Plot original data & SMOTE in embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca428089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f4f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(DATAPATH, 'outputs', 'clusterimages3', f'{methods}_{run}_umap_hdbscan_cluster_label')\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "img_path = os.path.join(base_dir, 'images_knee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0159ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in df['cluster_label'].unique():\n",
    "    ids = df[df['cluster_label'] == cluster]['id'].tolist()\n",
    "    cluster_dir = os.path.join(output_path, f'cluster_{cluster}')\n",
    "    os.makedirs(cluster_dir, exist_ok=True)\n",
    "    for id_ in ids:\n",
    "        img_file = os.path.join(img_path, f'{id_}.png')\n",
    "        if os.path.exists(img_file):\n",
    "            dest_file = os.path.join(cluster_dir, f'{id_}.png')\n",
    "            if not os.path.exists(dest_file):\n",
    "                shutil.copy2(img_file, dest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2686fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path2 = os.path.join(DATAPATH, 'outputs', 'clusterimages3', f'{methods}_{run}_umap_hdbscan_old_label')\n",
    "os.makedirs(output_path2, exist_ok=True)\n",
    "img_path = os.path.join(base_dir, 'images_knee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c2625",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in df['old_cluster'].unique():\n",
    "    ids = df[df['old_cluster'] == cluster]['id'].tolist()\n",
    "    cluster_dir = os.path.join(output_path2, f'cluster_{cluster}')\n",
    "    os.makedirs(cluster_dir, exist_ok=True)\n",
    "    for id_ in ids:\n",
    "        img_file = os.path.join(img_path, f'{id_}.png')\n",
    "        if os.path.exists(img_file):\n",
    "            dest_file = os.path.join(cluster_dir, f'{id_}.png')\n",
    "            if not os.path.exists(dest_file):\n",
    "                shutil.copy2(img_file, dest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354de709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdbscan_doc = pd.read_csv(os.path.join(filepath, f'pipeline_run10_umap_hdbscan_scaled_allpoints_wKL.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e33953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdbscan_doc['clusterlabelold'] = hdbscan_doc['cluster_label']\n",
    "# hdbscan_doc['clusterlabeltext'] = hdbscan_doc['cluster_label'].astype(str).map(rating)\n",
    "# hdbscan_doc['cluster_label'] = hdbscan_doc['clusterlabeltext'].map(rating_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31130b",
   "metadata": {},
   "source": [
    "# Get images per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8471e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def get_image_id(img_path):\n",
    "    return os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "\n",
    "def show_cluster_examples(output_path, df, max_grid=3, seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Build KL lookup\n",
    "    kl_lookup = dict(zip(df['name'], df['KL-Score']))\n",
    "\n",
    "    clusters = sorted([c for c in os.listdir(output_path) if c.startswith(\"cluster_\")])\n",
    "    img_names_l = []\n",
    "\n",
    "    for cluster in clusters:\n",
    "        cluster_dir = os.path.join(output_path, cluster)\n",
    "\n",
    "        imgs = [\n",
    "            os.path.join(cluster_dir, f)\n",
    "            for f in os.listdir(cluster_dir)\n",
    "            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "        ]\n",
    "\n",
    "        if len(imgs) == 0:\n",
    "            print(f\"No images found in {cluster}\")\n",
    "            continue\n",
    "\n",
    "        # -----------------------------\n",
    "        # Group images by KL-score\n",
    "        # -----------------------------\n",
    "        imgs_by_kl = {}\n",
    "        for img_path in imgs:\n",
    "            img_id = get_image_id(img_path)\n",
    "            kl = kl_lookup.get(img_id, None)\n",
    "            if kl is None:\n",
    "                continue\n",
    "            imgs_by_kl.setdefault(kl, []).append(img_path)\n",
    "\n",
    "        available_kls = sorted(imgs_by_kl.keys())\n",
    "\n",
    "        if len(available_kls) == 0:\n",
    "            print(f\"No KL-labelled images in {cluster}\")\n",
    "            continue\n",
    "\n",
    "        # -----------------------------\n",
    "        # Ensure at least one per KL\n",
    "        # -----------------------------\n",
    "        sample_imgs = []\n",
    "\n",
    "        for kl in available_kls:\n",
    "            sample_imgs.append(random.choice(imgs_by_kl[kl]))\n",
    "\n",
    "        # -----------------------------\n",
    "        # Fill remaining slots (if any)\n",
    "        # -----------------------------\n",
    "        max_images = max_grid * max_grid\n",
    "        remaining = max_images - len(sample_imgs)\n",
    "\n",
    "        if remaining > 0:\n",
    "            remaining_imgs = [\n",
    "                img for img in imgs\n",
    "                if img not in sample_imgs\n",
    "            ]\n",
    "\n",
    "            if len(remaining_imgs) > 0:\n",
    "                sample_imgs.extend(\n",
    "                    random.sample(\n",
    "                        remaining_imgs,\n",
    "                        min(remaining, len(remaining_imgs))\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        img_names_l.append(sample_imgs)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Plot\n",
    "        # -----------------------------\n",
    "        n_images = len(sample_imgs)\n",
    "        cols = min(max_grid, n_images)\n",
    "        rows = (n_images + cols - 1) // cols\n",
    "\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "        fig.suptitle(\n",
    "            f\"Cluster {cluster.replace('cluster_', '')}: Example Images\",\n",
    "            fontsize=18\n",
    "        )\n",
    "\n",
    "        if rows == 1 and cols == 1:\n",
    "            axes = [axes]\n",
    "        elif rows == 1:\n",
    "            axes = axes\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "\n",
    "        for ax, img_path in zip(axes, sample_imgs):\n",
    "            img = Image.open(img_path).convert(\"L\")\n",
    "            ax.imshow(img, cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "            img_id = get_image_id(img_path)\n",
    "            kl = kl_lookup.get(img_id, \"KL ?\")\n",
    "\n",
    "            ax.text(\n",
    "                0.02, 0.02,\n",
    "                f\"KL {kl}\",\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=14,\n",
    "                color=\"white\",\n",
    "                ha=\"left\",\n",
    "                va=\"bottom\",\n",
    "                bbox=dict(facecolor=\"black\", alpha=0.5, pad=2)\n",
    "            )\n",
    "\n",
    "        # Hide unused axes\n",
    "        for ax in axes[len(sample_imgs):]:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    #return img_names_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f167a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_examples(output_path, df, max_grid=3, seed=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36735c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_examples(output_path2, df, max_grid=3, seed=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
